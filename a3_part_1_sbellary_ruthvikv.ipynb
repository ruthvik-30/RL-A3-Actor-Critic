{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cf4d8c9b006ada45",
      "metadata": {
        "id": "cf4d8c9b006ada45"
      },
      "source": [
        "## <center>CSE 546: Reinforcement Learning</center>\n",
        "### <center>Prof. Alina Vereshchaka</center>\n",
        "#### <center>Spring 2025</center>\n",
        "\n",
        "Welcome to the Assignment 3, Part 1: Introduction to Actor-Critic Methods! It includes the implementation of simple actor and critic networks and best practices used in modern Actor-Critic algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d7a6d891e2fb312",
      "metadata": {
        "id": "9d7a6d891e2fb312"
      },
      "source": [
        "## Section 0: Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "53473293aa9daf8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53473293aa9daf8e",
        "outputId": "45f4df5f-ed75-455e-f95c-0ca9fbcac07e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ab8c5ace170>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gymnasium\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a3d9c34ff222994",
      "metadata": {
        "id": "2a3d9c34ff222994"
      },
      "source": [
        "## Section 1: Actor-Critic Network Architectures and Loss Computation\n",
        "\n",
        "In this section, you will explore two common architectural designs for Actor-Critic methods and implement their corresponding loss functions using dummy tensors. These architectures are:\n",
        "- A. Completely separate actor and critic networks\n",
        "- B. A shared network with two output heads\n",
        "\n",
        "Both designs are widely used in practice. Shared networks are often more efficient and generalize better, while separate networks offer more control and flexibility.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "971fa7887dd4f858",
      "metadata": {
        "id": "971fa7887dd4f858"
      },
      "source": [
        "### Task 1a – Separate Actor and Critic Networks with Loss Function\n",
        "\n",
        "Define a class `SeparateActorCritic`. Your goal is to:\n",
        "- Create two completely independent neural networks: one for the actor and one for the critic.\n",
        "- The actor should output a probability distribution over discrete actions (use `nn.Softmax`).\n",
        "- The critic should output a single scalar value.\n",
        "\n",
        " Use `nn.ReLU()` as your activation function. Include at least one hidden layer of reasonable width (e.g. 64 or 128 units).\n",
        "\n",
        "```python\n",
        "# TODO: Define SeparateActorCritic class\n",
        "```\n",
        "\n",
        " Next, simulate training using dummy tensors:\n",
        "1. Generate dummy tensors for log-probabilities, returns, estimated values, and entropies.\n",
        "2. Compute the actor loss using the advantage (return - value).\n",
        "3. Compute the critic loss as mean squared error between values and returns.\n",
        "4. Use a single optimizer for both the Actor and the Critic. In this case, combine the actor and critic losses into a total loss and perform backpropagation.\n",
        "5. Use a separate optimizers for both the Actor and the Critic. In this case, keep the actor and critic losses separate and perform backpropagation.\n",
        "\n",
        "```python\n",
        "# TODO: Simulate loss computation and backpropagation\n",
        "```\n",
        "\n",
        "🔗 Helpful references:\n",
        "- PyTorch Softmax: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
        "- PyTorch MSE Loss: https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dd6b81ed1791e4e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd6b81ed1791e4e6",
        "outputId": "c8cbb1b0-4f90-4436-9a93-38ae26eee742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joint optimizer step completed.\n",
            "Actor Loss: 0.3700183928012848\n",
            "Critic Loss: 0.1863611489534378\n",
            "Separate optimizers step completed.\n",
            "Actor Loss: 0.4754375219345093\n",
            "Critic Loss: 0.1904817372560501\n"
          ]
        }
      ],
      "source": [
        "# TODO: Define a class SeparateActorCritic with separate networks for actor and critic\n",
        "\n",
        "# BEGIN_YOUR_CODE\n",
        "\n",
        "class SeparateActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, action_dim):\n",
        "        super(SeparateActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.actor(x), self.critic(x)\n",
        "\n",
        "# Dummy settings\n",
        "input_dim = 8\n",
        "hidden_dim = 128\n",
        "action_dim = 4\n",
        "batch_size = 5\n",
        "dummy_states = torch.randn(batch_size, input_dim)\n",
        "dummy_returns = torch.randn(batch_size, 1)\n",
        "\n",
        "#JOINT OPTIMIZER\n",
        "model1 = SeparateActorCritic(input_dim, hidden_dim, action_dim)\n",
        "\n",
        "policy, values = model1(dummy_states)\n",
        "log_probs = torch.log(policy.gather(1, torch.randint(0, action_dim, (batch_size, 1))))\n",
        "entropy = -(policy * torch.log(policy + 1e-8)).sum(dim=1, keepdim=True)\n",
        "advantage = dummy_returns - values.detach()\n",
        "\n",
        "actor_loss = -(log_probs * advantage).mean() - 0.01 * entropy.mean()\n",
        "critic_loss = F.mse_loss(values, dummy_returns)\n",
        "\n",
        "optimizer_joint = optim.Adam(list(model1.actor.parameters()) + list(model1.critic.parameters()), lr=1e-3)\n",
        "optimizer_joint.zero_grad()\n",
        "(actor_loss + critic_loss).backward()\n",
        "optimizer_joint.step()\n",
        "\n",
        "print(\"Joint optimizer step completed.\")\n",
        "print(\"Actor Loss:\", actor_loss.item())\n",
        "print(\"Critic Loss:\", critic_loss.item())\n",
        "\n",
        "#SEPARATE OPTIMIZERS\n",
        "model2 = SeparateActorCritic(input_dim, hidden_dim, action_dim)\n",
        "\n",
        "policy, values = model2(dummy_states)\n",
        "log_probs = torch.log(policy.gather(1, torch.randint(0, action_dim, (batch_size, 1))))\n",
        "entropy = -(policy * torch.log(policy + 1e-8)).sum(dim=1, keepdim=True)\n",
        "advantage = dummy_returns - values.detach()\n",
        "\n",
        "actor_loss = -(log_probs * advantage).mean() - 0.01 * entropy.mean()\n",
        "critic_loss = F.mse_loss(values, dummy_returns)\n",
        "\n",
        "optimizer_actor = optim.Adam(model2.actor.parameters(), lr=1e-3)\n",
        "optimizer_critic = optim.Adam(model2.critic.parameters(), lr=1e-3)\n",
        "\n",
        "# Update actor\n",
        "optimizer_actor.zero_grad()\n",
        "actor_loss.backward(retain_graph=True)\n",
        "optimizer_actor.step()\n",
        "\n",
        "# Update critic\n",
        "optimizer_critic.zero_grad()\n",
        "critic_loss.backward()\n",
        "optimizer_critic.step()\n",
        "\n",
        "print(\"Separate optimizers step completed.\")\n",
        "print(\"Actor Loss:\", actor_loss.item())\n",
        "print(\"Critic Loss:\", critic_loss.item())\n",
        "\n",
        "\n",
        "# END_YOUR_CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb8e90c88108cd2e",
      "metadata": {
        "id": "eb8e90c88108cd2e"
      },
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Joint Optimizer: One Optimizer for Actor & Critic**\n",
        "\n",
        "#### **Motivation**:\n",
        "- **Simplifies training**: Only one optimizer to manage.\n",
        "- **Efficient memory usage**: Shared optimizer state (like moment estimates in Adam).\n",
        "- **Synchronized updates**: Both networks are updated together in one pass.\n",
        "\n",
        "#### **When to Use**:\n",
        "- When the actor and critic networks are tightly coupled or **share parameters** (e.g., shared CNN base in some actor-critic implementations).\n",
        "- When you want a simpler training loop.\n",
        "- When both networks benefit from **shared learning rates** and optimization dynamics.\n",
        "\n",
        "### **Separate Optimizers: One for Actor, One for Critic**\n",
        "\n",
        "#### **Motivation**:\n",
        "- **Decouples learning dynamics**: You can control learning rates, weight decay, and schedulers **independently**.\n",
        "- More **fine-grained control** over training stability, especially in complex environments.\n",
        "- Better when actor and critic have **different convergence behavior**.\n",
        "\n",
        "#### **When to Use**:\n",
        "- In large-scale or high-stakes environments (e.g., PPO, A3C) where **training stability** is critical.\n",
        "- When the actor and critic are entirely separate networks (like in your setup).\n",
        "- If you want to **freeze** one of the networks at any point (e.g., to prevent value overfitting)."
      ],
      "metadata": {
        "id": "wOgTioL4a4dL"
      },
      "id": "wOgTioL4a4dL"
    },
    {
      "cell_type": "markdown",
      "id": "64081a606b93029d",
      "metadata": {
        "id": "64081a606b93029d"
      },
      "source": [
        "### Task 1b – Shared Network with Actor and Critic Heads + Loss Function\n",
        "\n",
        "Now define a class `SharedActorCritic`:\n",
        "- Build a shared base network (e.g., linear layer + ReLU)\n",
        "- Create two heads: one for actor (output action probabilities) and one for critic (output state value)\n",
        "\n",
        "```python\n",
        "# TODO: Define SharedActorCritic class\n",
        "```\n",
        "\n",
        "Then:\n",
        "1. Pass a dummy input tensor through the model to obtain action probabilities and value.\n",
        "2. Simulate dummy rewards and compute advantage.\n",
        "3. Compute the actor and critic losses, combine them, and backpropagate.\n",
        "\n",
        "```python\n",
        "# TODO: Simulate shared network loss computation and backpropagation\n",
        "```\n",
        "\n",
        " Use `nn.Softmax` for actor output and `nn.Linear` for scalar critic output.\n",
        "\n",
        "🔗 More reading:\n",
        "- Policy Gradient Methods: https://spinningup.openai.com/en/latest/algorithms/vpg.html\n",
        "- Actor-Critic Overview: https://www.tensorflow.org/agents/tutorials/6_reinforce_tutorial\n",
        "- PyTorch Categorical Distribution: https://pytorch.org/docs/stable/distributions.html#categorical\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a48f882fff11aecc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a48f882fff11aecc",
        "outputId": "c88aeb80-29e7-468d-b76f-f1e099f3a89a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shared network training step complete.\n",
            "Actor Loss: 0.1559678316116333\n",
            "Critic Loss: 0.3759419322013855\n"
          ]
        }
      ],
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "class SharedActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, action_dim):\n",
        "        super(SharedActorCritic, self).__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.actor_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic_head = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        base = self.shared(x)\n",
        "        probs = self.actor_head(base)\n",
        "        value = self.critic_head(base)\n",
        "        return probs, value\n",
        "\n",
        "input_dim = 8\n",
        "hidden_dim = 128\n",
        "action_dim = 4\n",
        "batch_size = 5\n",
        "\n",
        "dummy_states = torch.randn(batch_size, input_dim)\n",
        "dummy_returns = torch.randn(batch_size, 1)\n",
        "model = SharedActorCritic(input_dim, hidden_dim, action_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "probs, values = model(dummy_states)\n",
        "actions = torch.randint(0, action_dim, (batch_size, 1))\n",
        "log_probs = torch.log(probs.gather(1, actions))\n",
        "entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=1, keepdim=True)\n",
        "advantage = dummy_returns - values.detach()\n",
        "\n",
        "actor_loss = -(log_probs * advantage).mean() - 0.01 * entropy.mean()\n",
        "critic_loss = F.mse_loss(values, dummy_returns)\n",
        "total_loss = actor_loss + critic_loss\n",
        "optimizer.zero_grad()\n",
        "total_loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Shared network training step complete.\")\n",
        "print(\"Actor Loss:\", actor_loss.item())\n",
        "print(\"Critic Loss:\", critic_loss.item())\n",
        "\n",
        "\n",
        "# END_YOUR_CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a974e302d1fdb028",
      "metadata": {
        "id": "a974e302d1fdb028"
      },
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Shared Network**\n",
        "\n",
        "#### **Motivation & Benefits:**\n",
        "- **Efficient representation**: A shared base helps learn common features from the state, saving computation.\n",
        "- **Parameter efficiency**: Fewer parameters = faster training, better for memory-limited scenarios.\n",
        "- **Better gradient flow**: Shared features are updated from both actor and critic gradients—can speed up convergence.\n",
        "\n",
        "#### **When to use:**\n",
        "- In environments where actor and critic benefit from **shared understanding of the state**.\n",
        "- When training needs to be fast or memory-efficient (e.g., on-device RL).\n",
        "- Common in **vanilla A2C, A3C, PPO**."
      ],
      "metadata": {
        "id": "D32Gyqqqb7g6"
      },
      "id": "D32Gyqqqb7g6"
    },
    {
      "cell_type": "markdown",
      "id": "eb645eb009b85b1c",
      "metadata": {
        "id": "eb645eb009b85b1c"
      },
      "source": [
        "## Section 2: Auto-Adaptive Network Setup for Environments\n",
        "\n",
        "You will now create a function that builds a shared actor-critic network that adapts to any Gymnasium environment. This function should inspect the environment and build input/output layers accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4223b6ddf43abee5",
      "metadata": {
        "id": "4223b6ddf43abee5"
      },
      "source": [
        "### Task 2: Auto-generate Input and Output Layers\n",
        "Write a function `create_shared_network(env)` that constructs a neural network using the following rules:\n",
        "- The input layer should match the environment's observation space.\n",
        "- The output layer for the **actor** should depend on the action space:\n",
        "  - For discrete actions: output probabilities using `nn.Softmax`.\n",
        "  - For continuous actions: output mean and log std for a Gaussian distribution.\n",
        "- The **critic** always outputs a single scalar value.\n",
        "\n",
        "```python\n",
        "# TODO: Define function `create_shared_network(env)`\n",
        "```\n",
        "\n",
        "#### Environments to Support:\n",
        "Test your function with the following environments:\n",
        "1. `CliffWalking-v0` (Use one-hot encoding for discrete integer observations.)\n",
        "2. `LunarLander-v3` (Standard Box space for observations and discrete actions.)\n",
        "3. `PongNoFrameskip-v4` (Use gym wrappers for Atari image preprocessing.)\n",
        "4. `HalfCheetah-v5` (Continuous observation and continuous action.)\n",
        "\n",
        "```python\n",
        "# TODO: Loop through environments and test `create_shared_network`\n",
        "```\n",
        "\n",
        "Hint: Use `gym.spaces` utilities to determine observation/action types dynamically.\n",
        "\n",
        "🔗 Observation/Action Space Docs:\n",
        "- https://gymnasium.farama.org/api/spaces/\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\"\n",
        "!pip install \"gymnasium[mujoco]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EayUlz3RlKuO",
        "outputId": "19a3386d-9baa-434b-fd35-bcc00e5d372f"
      },
      "id": "EayUlz3RlKuO",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.9 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.0\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379449 sha256=b019f6d5446714dba636cd97364e8915bda9a19bb60173e08a110e16a8785df6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n",
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[mujoco])\n",
            "  Downloading glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n",
            "Downloading mujoco-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.9.0 mujoco-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "d6d249ff9277403a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6d249ff9277403a",
        "outputId": "27e58882-565a-4aa8-cd56-8dbd1d47f66c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ CliffWalking-v0 passed.\n",
            "  Actor Output: tensor([[0.2403, 0.2580, 0.2575, 0.2442]])\n",
            "  Critic Output: 0.013050481677055359\n",
            "\n",
            "✅ LunarLander-v3 passed.\n",
            "  Actor Output: tensor([[0.2480, 0.2451, 0.2258, 0.2811]])\n",
            "  Critic Output: -0.0473642498254776\n",
            "\n",
            "❌ PongNoFrameskip-v4 failed: Environment `PongNoFrameskip` doesn't exist.\n",
            "✅ HalfCheetah-v5 passed.\n",
            "  Actor Output: (tensor([[ 0.0835,  0.0516, -0.0133,  0.0180, -0.0043,  0.0262]]), tensor([[0., 0., 0., 0., 0., 0.]], requires_grad=True))\n",
            "  Critic Output: -0.011038020253181458\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from gymnasium.spaces import Discrete, Box\n",
        "from torchvision import transforms\n",
        "\n",
        "class SharedNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, action_space):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.action_space = action_space\n",
        "\n",
        "        if isinstance(action_space, Discrete):\n",
        "            self.actor = nn.Sequential(\n",
        "                nn.Linear(hidden_dim, action_space.n),\n",
        "                nn.Softmax(dim=-1)\n",
        "            )\n",
        "        elif isinstance(action_space, Box):\n",
        "            self.actor_mean = nn.Linear(hidden_dim, action_space.shape[0])\n",
        "            self.actor_log_std = nn.Parameter(torch.zeros(action_space.shape[0]))\n",
        "        else:\n",
        "            raise NotImplementedError(\"Unsupported action space.\")\n",
        "\n",
        "        self.critic = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.shared(x)\n",
        "        if isinstance(self.action_space, Discrete):\n",
        "            return self.actor(x), self.critic(x)\n",
        "        elif isinstance(self.action_space, Box):\n",
        "            mean = self.actor_mean(x)\n",
        "            log_std = self.actor_log_std.expand_as(mean)\n",
        "            return (mean, log_std), self.critic(x)\n",
        "\n",
        "\n",
        "def create_shared_network(env, hidden_dim=128):\n",
        "    obs_space = env.observation_space\n",
        "    act_space = env.action_space\n",
        "\n",
        "    if isinstance(obs_space, Discrete):\n",
        "        input_dim = obs_space.n\n",
        "    elif isinstance(obs_space, Box):\n",
        "        input_dim = int(np.prod(obs_space.shape))\n",
        "    else:\n",
        "        raise NotImplementedError(\"Unsupported observation space.\")\n",
        "\n",
        "    return SharedNetwork(input_dim, hidden_dim, act_space)\n",
        "\n",
        "\n",
        "def preprocess_obs(obs, obs_space):\n",
        "    if isinstance(obs_space, Discrete):\n",
        "        return torch.eye(obs_space.n)[obs].float().unsqueeze(0)\n",
        "    elif isinstance(obs_space, Box):\n",
        "        return torch.tensor(obs, dtype=torch.float32).flatten().unsqueeze(0)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "env_ids = [\n",
        "    \"CliffWalking-v0\",       # Gymnasium\n",
        "    \"LunarLander-v3\",        # Gymnasium\n",
        "    \"PongNoFrameskip-v4\",    # Gym\n",
        "    \"HalfCheetah-v5\"         # Gymnasium\n",
        "]\n",
        "\n",
        "for env_id in env_ids:\n",
        "    try:\n",
        "        env = gymnasium.make(env_id)\n",
        "        obs, _ = env.reset()\n",
        "        model = create_shared_network(env)\n",
        "        obs_tensor = preprocess_obs(obs, env.observation_space)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(obs_tensor)\n",
        "\n",
        "        print(f\"✅ {env_id} passed.\")\n",
        "        print(\"  Actor Output:\", output[0])\n",
        "        print(\"  Critic Output:\", output[1].item())\n",
        "        print()\n",
        "\n",
        "        env.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ {env_id} failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, PongNoFrameskip-v4 environment does not exist because we are using gymnasium and PongNoFrameskip-v4 exists in gym. So, I have run PongNoFrameskip-v4 in the below cell."
      ],
      "metadata": {
        "id": "5NYBJC3qjnwb"
      },
      "id": "5NYBJC3qjnwb"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5\n",
        "!pip install \"gym[atari,accept-rom-license]\" ale-py autorom\n",
        "!AutoROM --accept-license\n",
        "import gym\n",
        "\n",
        "print(gym.envs.registry.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WSOnSH-7VyBh",
        "outputId": "e6749c46-15ec-4455-99bd-9adf1edd9d2a"
      },
      "id": "WSOnSH-7VyBh",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Collecting autorom\n",
            "  Using cached AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym[accept-rom-license,atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym[accept-rom-license,atari]) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "  Using cached AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "INFO: pip is looking at multiple versions of gym[accept-rom-license,atari] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting gym[accept-rom-license,atari]\n",
            "  Using cached gym-0.26.2.tar.gz (721 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py\n",
            "  Downloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ale-py) (6.5.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom) (8.1.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from autorom) (4.67.1)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (2025.1.31)\n",
            "Downloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827731 sha256=88909177e845488767c27029c424b49a1198e2448a838d9cf84cdaffb12918c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/77/9e/9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=autorom_accept_rom_license-0.6.1-py3-none-any.whl size=446711 sha256=964c48d7fe30e8c0400f82f4880afeeca01cc072646d8a1a9b00eb4287f1647e\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/fc/c6/8aa657c0d2089982f2dabd110efc68c61eb49831fdb7397351\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: gym, ale-py, AutoROM.accept-rom-license, autorom\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Attempting uninstall: ale-py\n",
            "    Found existing installation: ale-py 0.10.2\n",
            "    Uninstalling ale-py-0.10.2:\n",
            "      Successfully uninstalled ale-py-0.10.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires ale-py>=0.10.1, but you have ale-py 0.8.1 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 gym-0.26.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              },
              "id": "4c0b4329b0c44f879bb634abbcd61717"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.11/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/adventure.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/air_raid.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/alien.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/amidar.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/assault.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/asterix.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/asteroids.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/atlantis.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/atlantis2.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/backgammon.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/bank_heist.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/basic_math.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/battle_zone.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/beam_rider.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/berzerk.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/blackjack.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/bowling.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/boxing.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/breakout.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/carnival.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/casino.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/centipede.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/chopper_command.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/combat.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/crazy_climber.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/crossbow.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/darkchambers.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/defender.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/demon_attack.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/donkey_kong.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/double_dunk.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/earthworld.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/elevator_action.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/enduro.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/entombed.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/et.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/fishing_derby.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/flag_capture.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/freeway.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/frogger.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/frostbite.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/galaxian.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/gopher.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/gravitar.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/hangman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/haunted_house.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/hero.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/human_cannonball.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/ice_hockey.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/jamesbond.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/journey_escape.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/joust.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kaboom.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kangaroo.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/keystone_kapers.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/king_kong.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/klax.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/koolaid.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/krull.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kung_fu_master.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/laser_gates.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/lost_luggage.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/mario_bros.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/maze_craze.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/miniature_golf.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/montezuma_revenge.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/mr_do.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/ms_pacman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/name_this_game.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/othello.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pacman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/phoenix.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pitfall.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pitfall2.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pong.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pooyan.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/private_eye.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/qbert.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/riverraid.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/road_runner.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/robotank.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/seaquest.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/sir_lancelot.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/skiing.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/solaris.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/space_invaders.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/space_war.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/star_gunner.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/superman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/surround.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tennis.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tetris.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tic_tac_toe_3d.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/time_pilot.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/trondead.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/turmoil.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tutankham.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/up_n_down.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/venture.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_checkers.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_chess.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_cube.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_pinball.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/warlords.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/wizard_of_wor.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/word_zapper.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/yars_revenge.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/zaxxon.bin\n",
            "Done!\n",
            "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v2', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'Reacher-v2', 'Reacher-v4', 'Pusher-v2', 'Pusher-v4', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'HumanoidStandup-v2', 'HumanoidStandup-v4'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.wrappers import FrameStack, GrayScaleObservation, ResizeObservation\n",
        "\n",
        "def make_pong_env(env_id=\"PongNoFrameskip-v4\"):\n",
        "    env = gym.make(env_id)\n",
        "    env = gym.wrappers.AtariPreprocessing(env, grayscale_obs=True, scale_obs=False, frame_skip=4)\n",
        "    env = ResizeObservation(env, shape=84)\n",
        "    env = FrameStack(env, num_stack=4)\n",
        "    return env\n",
        "\n",
        "class SharedNetwork(nn.Module):\n",
        "    def __init__(self, input_shape, action_space):\n",
        "        super(SharedNetwork, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),  # from DeepMind Nature paper\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_output(input_shape)\n",
        "        self.fc = nn.Linear(conv_out_size, 512)\n",
        "\n",
        "        # Actor\n",
        "        if isinstance(action_space, gym.spaces.Discrete):\n",
        "            self.actor = nn.Linear(512, action_space.n)\n",
        "        else:\n",
        "            self.actor_mean = nn.Linear(512, action_space.shape[0])\n",
        "            self.actor_log_std = nn.Parameter(torch.zeros(action_space.shape[0]))\n",
        "\n",
        "        # Critic\n",
        "        self.critic = nn.Linear(512, 1)\n",
        "\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def _get_conv_output(self, shape):\n",
        "        o = torch.zeros(1, *shape)\n",
        "        o = self.conv(o)\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x / 255.0\n",
        "        x = x.squeeze(-1)\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc(x))\n",
        "\n",
        "        value = self.critic(x)\n",
        "\n",
        "        if isinstance(self.action_space, gym.spaces.Discrete):\n",
        "            logits = self.actor(x)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            return probs, value\n",
        "        else:\n",
        "            mean = self.actor_mean(x)\n",
        "            log_std = self.actor_log_std.expand_as(mean)\n",
        "            return (mean, log_std), value\n",
        "\n",
        "def create_shared_network(env):\n",
        "    input_shape = env.observation_space.shape\n",
        "    action_space = env.action_space\n",
        "    return SharedNetwork(input_shape, action_space)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = make_pong_env()\n",
        "    net = create_shared_network(env)\n",
        "\n",
        "    obs = env.reset()\n",
        "    obs = obs[0]\n",
        "    obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        actor_out, critic_out = net(obs)\n",
        "\n",
        "    print(\"Actor Output:\", actor_out)\n",
        "    print(\"Critic Output:\", critic_out.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU2SLMF3VkcT",
        "outputId": "41f9e4af-a13a-45a1-f916-fa6310ba01dc"
      },
      "id": "lU2SLMF3VkcT",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actor Output: tensor([[0.1694, 0.1722, 0.1685, 0.1668, 0.1615, 0.1615]])\n",
            "Critic Output: -0.004982149228453636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gymnasium.envs.registry.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojinz8-Zecyg",
        "outputId": "d3562f5e-f5ab-4aa7-ee18-8efa70024abf"
      },
      "id": "Ojinz8-Zecyg",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'phys2d/CartPole-v0', 'phys2d/CartPole-v1', 'phys2d/Pendulum-v0', 'LunarLander-v3', 'LunarLanderContinuous-v3', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v3', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'tabular/Blackjack-v0', 'tabular/CliffWalking-v0', 'Reacher-v2', 'Reacher-v4', 'Reacher-v5', 'Pusher-v2', 'Pusher-v4', 'Pusher-v5', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedPendulum-v5', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'InvertedDoublePendulum-v5', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'HalfCheetah-v5', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Hopper-v5', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Swimmer-v5', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Walker2d-v5', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Ant-v5', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'Humanoid-v5', 'HumanoidStandup-v2', 'HumanoidStandup-v4', 'HumanoidStandup-v5', 'GymV21Environment-v0', 'GymV26Environment-v0'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ccd13f0b62b30ff",
      "metadata": {
        "id": "4ccd13f0b62b30ff"
      },
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Discrete Observations (e.g., `CliffWalking-v0`)**\n",
        "- **Setup:**\n",
        "  - Observation space is `Discrete(n)` → Use **one-hot encoding**.\n",
        "  - Action space is `Discrete(k)` → Actor outputs a **Softmax distribution** over actions.\n",
        "- **Motivation:**\n",
        "  - One-hot encoding is a simple and efficient way to embed discrete states.\n",
        "  - Softmax makes the actor output interpretable as a probability distribution, suitable for categorical policies (e.g., in REINFORCE or PPO).\n",
        "- **Use Case:**\n",
        "  - Classic tabular-style environments like grid worlds, where each state is a discrete index.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Continuous Observations + Discrete Actions (e.g., `LunarLander-v3`)**\n",
        "- **Setup:**\n",
        "  - Observation space is a `Box` → Flatten and feed to a fully connected layer.\n",
        "  - Action space is `Discrete(k)` → Actor uses **Softmax** over logits.\n",
        "- **Motivation:**\n",
        "  - Many physics-based simulators use continuous observations (e.g., positions, velocities).\n",
        "  - Discrete actions still apply, so a classification-style output is appropriate.\n",
        "- **Use Case:**\n",
        "  - Great for problems where state features are continuous but actions are discrete (robotics, lunar lander, car control).\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Preprocessed Image Inputs + Discrete Actions (e.g., `PongNoFrameskip-v4`)**\n",
        "- **Setup:**\n",
        "  - Use **Atari wrappers** (e.g., grayscale, resize, frame stack) to get a 4x84x84 input.\n",
        "  - CNN → Flatten → FC → Actor (softmax) + Critic.\n",
        "- **Motivation:**\n",
        "  - CNNs are essential for spatial and temporal feature extraction from pixel input.\n",
        "  - Atari games have discrete actions, so Softmax fits well.\n",
        "- **Use Case:**\n",
        "  - Image-based environments where the agent perceives the world visually (e.g., Atari, vision-based robotics).\n",
        "  - This architecture aligns with the DeepMind DQN and A3C setups.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Continuous Observations + Continuous Actions (e.g., `HalfCheetah-v5`)**\n",
        "- **Setup:**\n",
        "  - Observation space is a `Box` → Flatten → FC layers.\n",
        "  - Action space is `Box` → Actor outputs:\n",
        "    - Mean vector via a Linear layer.\n",
        "    - `log_std` as a learnable parameter (shared across samples or optionally per-sample).\n",
        "- **Motivation:**\n",
        "  - For continuous action policies, the agent samples from a **Gaussian distribution**, parameterized by `mean` and `std`.\n",
        "  - This is common in **policy gradient methods** like PPO, TRPO, SAC.\n",
        "- **Use Case:**\n",
        "  - Locomotion and control tasks with precise force/torque outputs (e.g., MuJoCo, PyBullet).\n",
        "\n"
      ],
      "metadata": {
        "id": "4gvhUUKtjVmk"
      },
      "id": "4gvhUUKtjVmk"
    },
    {
      "cell_type": "markdown",
      "id": "b39c886fa536a639",
      "metadata": {
        "id": "b39c886fa536a639"
      },
      "source": [
        "### Task 3: Write Observation Normalization Function\n",
        "Create a function `normalize_observation(obs, env)` that:\n",
        "- Checks if the observation space is `Box` and has `low` and `high` attributes.\n",
        "- If so, normalize the input observation.\n",
        "- Otherwise, return the observation unchanged.\n",
        "\n",
        "```python\n",
        "# TODO: Define `normalize_observation(obs, env)`\n",
        "```\n",
        "\n",
        "Test this function with observations from:\n",
        "- `LunarLander-v3`\n",
        "- `PongNoFrameskip-v4`\n",
        "\n",
        "Note: Atari observations are image arrays. Normalize pixel values to [0, 1]. For LunarLander-v3, the different elements in the observation vector have different ranges. Normalize them to [0, 1] using the `low` and `high` attributes of the observation space.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "fc7ee06112cf7d29",
      "metadata": {
        "id": "fc7ee06112cf7d29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254185b6-bad2-42d2-9c78-58c91eb13606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LunarLander-v3:\n",
            "Original: [ 0.00143003  1.401336    0.1448246  -0.4259648  -0.00165019 -0.03280495\n",
            "  0.          0.        ]\n",
            "Normalized: [0.500286   0.7802672  0.50724125 0.47870177 0.49986866 0.49835977\n",
            " 0.         0.        ]\n",
            "Range: (min = 0.0 , max = 0.7802672 )\n",
            "\n",
            "Image-based obs detected, scaling by 255.\n",
            "PongNoFrameskip-v4:\n",
            "Original dtype: uint8\n",
            "Normalized dtype: float32\n",
            "Shape: (4, 84, 84)\n",
            "Range: (min = 0.20392157 , max = 0.9254902 )\n"
          ]
        }
      ],
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "from gym.wrappers.atari_preprocessing import AtariPreprocessing\n",
        "\n",
        "def normalize_observation(obs, env):\n",
        "    obs_space = env.observation_space\n",
        "    if isinstance(obs_space, (gym.spaces.Box, gymnasium.spaces.Box)) and hasattr(obs_space, 'low') and hasattr(obs_space, 'high'):\n",
        "        if not isinstance(obs, np.ndarray):\n",
        "            obs = np.array(obs)\n",
        "\n",
        "        if np.issubdtype(obs.dtype, np.uint8):\n",
        "            print(\"\\nImage-based obs detected, scaling by 255.\")\n",
        "            return obs.astype(np.float32) / 255.0\n",
        "\n",
        "        obs = obs.astype(np.float32)\n",
        "        low = obs_space.low\n",
        "        high = obs_space.high\n",
        "        normalized = np.copy(obs)\n",
        "        finite_mask = np.isfinite(low) & np.isfinite(high)\n",
        "        if np.any(finite_mask):\n",
        "            normalized[finite_mask] = (\n",
        "                (obs[finite_mask] - low[finite_mask]) /\n",
        "                (high[finite_mask] - low[finite_mask] + 1e-8)\n",
        "            )\n",
        "\n",
        "        return normalized\n",
        "    return obs\n",
        "\n",
        "env_lunar = gymnasium.make(\"LunarLander-v3\")\n",
        "obs_lunar, _ = env_lunar.reset()\n",
        "norm_obs_lunar = normalize_observation(obs_lunar, env_lunar)\n",
        "\n",
        "print(\"LunarLander-v3:\")\n",
        "print(\"Original:\", obs_lunar)\n",
        "print(\"Normalized:\", norm_obs_lunar)\n",
        "print(\"Range: (min =\", np.min(norm_obs_lunar), \", max =\", np.max(norm_obs_lunar), \")\")\n",
        "\n",
        "env_pong = gym.make(\"PongNoFrameskip-v4\")\n",
        "env_pong = AtariPreprocessing(env_pong, frame_skip=1, scale_obs=False)\n",
        "env_pong = FrameStack(env_pong, 4)\n",
        "obs_pong, _ = env_pong.reset()\n",
        "obs_pong = np.array(obs_pong)\n",
        "norm_obs_pong = normalize_observation(obs_pong, env_pong)\n",
        "\n",
        "print(\"PongNoFrameskip-v4:\")\n",
        "print(\"Original dtype:\", obs_pong.dtype)\n",
        "print(\"Normalized dtype:\", norm_obs_pong.dtype)\n",
        "print(\"Shape:\", norm_obs_pong.shape)\n",
        "print(\"Range: (min =\", np.min(norm_obs_pong), \", max =\", np.max(norm_obs_pong), \")\")\n",
        "# END_YOUR_CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "501ed2a6e7ca7a7b",
      "metadata": {
        "id": "501ed2a6e7ca7a7b"
      },
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Motivation Behind the Normalization Function**\n",
        "\n",
        "Observation normalization is a **critical preprocessing step** for many reinforcement learning (RL) algorithms. The main goal is to ensure the input values fall within a standardized range, often `[0, 1]` or `[-1, 1]`. This improves:\n",
        "\n",
        "- **Stability of training**\n",
        "- **Faster convergence**\n",
        "- **Better generalization across environments**\n",
        "- **More balanced gradient updates**\n",
        "\n",
        "## **LunarLander-v3 (Low-Dimensional Vector Observations)**\n",
        "\n",
        "### **Observation Characteristics**\n",
        "- Observation: 8-dimensional continuous vector\n",
        "- Range: Varies across each dimension\n",
        "- Example: Position, velocity, angle, leg contacts\n",
        "\n",
        "### **Normalization Method**\n",
        "- Normalize each component using:\n",
        "  $\\text{normalized}_i = \\frac{x_i - \\text{low}_i}{\\text{high}_i - \\text{low}_i}$\n",
        "\n",
        "- Ensures all features are in the range `[0, 1]` if bounds are finite.\n",
        "\n",
        "### **Why This Matters**\n",
        "- Helps algorithms like **DQN, PPO, A2C** treat all features equally in magnitude.\n",
        "- Prevents features with larger numeric ranges from dominating others.\n",
        "- Especially useful when observation components vary greatly (e.g., positions vs. contact flags).\n",
        "\n",
        "### **When to Use**\n",
        "- For any **low-dimensional Box space** with well-defined bounds.\n",
        "- Ideal when using **MLPs** or other non-convolutional architectures.\n",
        "\n",
        "## **PongNoFrameskip-v4 (Image Observations)**\n",
        "\n",
        "### 🔍 **Observation Characteristics**\n",
        "- Observation: Stack of 4 grayscale frames (shape: `(4, 84, 84)`)\n",
        "- Data type: `uint8` (pixel values in `[0, 255]`)\n",
        "\n",
        "### **Normalization Method**\n",
        "- Divide by 255 to scale values to `[0, 1]`\n",
        "\n",
        "### **Why This Matters**\n",
        "- Neural networks (especially CNNs) work better with inputs in a standardized range.\n",
        "- Prevents large gradients from pixel values that are too high.\n",
        "- Improves learning efficiency and stability, especially for vision-based policies.\n",
        "\n",
        "### **When to Use**\n",
        "- Any time you're dealing with **image-based observations** (Atari, VizDoom, etc.)\n",
        "- Essential when using CNNs or pretrained models that expect normalized input\n"
      ],
      "metadata": {
        "id": "bPG2uyOgk-RC"
      },
      "id": "bPG2uyOgk-RC"
    },
    {
      "cell_type": "markdown",
      "id": "6b5fb5353307f514",
      "metadata": {
        "id": "6b5fb5353307f514"
      },
      "source": [
        "## Section 4: Gradient Clipping\n",
        "\n",
        "To prevent exploding gradients, it's common practice to clip gradients before optimizer updates.\n",
        "\n",
        "### Task 4: Clip Gradients for Actor-Critic Networks\n",
        "Use dummy tensors and apply gradient clipping with the following PyTorch method:\n",
        "```python\n",
        "# During training, after loss.backward():\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "```\n",
        "\n",
        "Reuse the loss computation from Task 1a or 1b. After computing the gradients, apply gradient clipping.\n",
        "Print the gradient norm before and after clipping to verify it’s applied.\n",
        "\n",
        "🔗 PyTorch Docs: https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "7327507fb6e803ad",
      "metadata": {
        "id": "7327507fb6e803ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81833cc1-c325-4a1d-8c2a-2a78d70d4a27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 1] Loss: 1.5469 | Grad Norm Before: 4.5488 | After: 0.5000\n",
            "[Step 2] Loss: 1.5651 | Grad Norm Before: 2.6962 | After: 0.5000\n",
            "[Step 3] Loss: 1.8967 | Grad Norm Before: 3.5366 | After: 0.5000\n",
            "[Step 4] Loss: 1.8945 | Grad Norm Before: 4.4850 | After: 0.5000\n",
            "[Step 5] Loss: 1.6763 | Grad Norm Before: 5.1891 | After: 0.5000\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.actor_fc = nn.Linear(128, output_dim)\n",
        "        self.critic_fc = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        action_logits = self.actor_fc(x)\n",
        "        state_value = self.critic_fc(x)\n",
        "        return action_logits, state_value\n",
        "\n",
        "input_dim = 8\n",
        "output_dim = 4\n",
        "model = ActorCritic(input_dim, output_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Dummy Training Loop\n",
        "for step in range(1, 6):\n",
        "    dummy_obs = torch.randn(1, input_dim)\n",
        "    dummy_action = torch.randint(0, output_dim, (1,))\n",
        "    dummy_return = torch.rand(1, 1)\n",
        "\n",
        "    logits, value = model(dummy_obs)\n",
        "\n",
        "    policy_loss = F.cross_entropy(logits, dummy_action)\n",
        "    value_loss = F.mse_loss(value, dummy_return)\n",
        "    loss = policy_loss + value_loss\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    #before clipping\n",
        "    total_norm_before = torch.norm(\n",
        "        torch.stack([p.grad.norm() for p in model.parameters() if p.grad is not None])\n",
        "    )\n",
        "\n",
        "    # Clip gradients\n",
        "    clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "\n",
        "    # after clipping\n",
        "    total_norm_after = torch.norm(\n",
        "        torch.stack([p.grad.norm() for p in model.parameters() if p.grad is not None])\n",
        "    )\n",
        "\n",
        "    optimizer.step()\n",
        "    print(f\"[Step {step}] Loss: {loss.item():.4f} | Grad Norm Before: {total_norm_before:.4f} | After: {total_norm_after:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9952750fa74cd487",
      "metadata": {
        "id": "9952750fa74cd487"
      },
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In deep reinforcement learning (especially with Actor-Critic architectures), gradients can sometimes **explode**, particularly:\n",
        "- When rewards are very large or highly variable\n",
        "- When the network becomes unstable due to off-policy learning or poor initial exploration\n",
        "- When using deep or recurrent networks with many layers\n",
        "\n",
        "#### Problem: Exploding Gradients\n",
        "- Large gradients → large parameter updates → unstable learning → diverging policy/value function\n",
        "\n",
        "#### Solution: Gradient Clipping\n",
        "This technique **caps the total gradient norm** to a fixed maximum (here, `0.5`) to ensure updates remain stable:\n"
      ],
      "metadata": {
        "id": "ebe908Jsm-bo"
      },
      "id": "ebe908Jsm-bo"
    },
    {
      "cell_type": "markdown",
      "id": "f4cff31e6c6e7e4a",
      "metadata": {
        "id": "f4cff31e6c6e7e4a"
      },
      "source": [
        "If you are working in a team, provide a contribution summary.\n",
        "| Team Member | Step# | Contribution (%) |\n",
        "|---|---|---|\n",
        "| Ruthvik Vasantha Kumar  | Task 1 | 100%  |\n",
        "| Ruthvik Vasantha Kumar  | Task 2 | 100%  |\n",
        "| Shreyas Bellary Manjunath  | Task 3 | 100%  |\n",
        "| Shreyas Bellary Manjunath  | Task 4 | 100%  |\n",
        "|   | **Total** | 100%  |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4be0a6e29f281e23",
      "metadata": {
        "id": "4be0a6e29f281e23"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}